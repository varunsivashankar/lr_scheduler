{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DoubleDescentResearch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrVktr98QGbt"
      },
      "source": [
        "**Helper Functions**\n",
        "\n",
        "Below are some helper functions for the simulations. This cell is required for both types of simulations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcQbFRP_uMZD"
      },
      "source": [
        "import numpy as np\n",
        "import progressbar\n",
        "import matplotlib.pyplot as plt\n",
        "import progressbar\n",
        "\n",
        "\"\"\"\n",
        "Generate training/testing data based on parameters.\n",
        " \n",
        "Parameters\n",
        "--------------------\n",
        "    n -- int, number of points in data set\n",
        "    d -- int, data dimension\n",
        "    W -- numpy array of shape (d,1), true weights of the data distribution\n",
        "Returns\n",
        "--------------------\n",
        "    X -- numpy array of shape (n,d), each row is a data point x_i ~ N(0,I_d)\n",
        "    Y -- numpy array of shape (n,1), each value is y_i ~ xW + noise\n",
        "\"\"\"\n",
        "def generateData(n,d,W):\n",
        "    X = np.random.normal(0,1, size=(n,d))\n",
        "    noise = np.random.normal(0,0.1,size=(n,1))\n",
        "    Y = np.dot(X,W) + noise\n",
        "    return [X,Y]\n",
        "\n",
        "\"\"\"\n",
        "Computes true value of critical learning rate\n",
        "\n",
        "Parameters\n",
        "--------------------\n",
        "    x      -- numpy array of shape (n,d)\n",
        "    y      -- numpy array of shape (n,1)\n",
        "    W      -- numpy array of shape (d,1), true weights of the data distribution\n",
        "    W_pred -- numpy array of shape (d,1), estimated weights of the data distribution\n",
        "Returns\n",
        "--------------------\n",
        "    alpha  -- float, critical value of the learning rate (computed using true weights)\n",
        "\"\"\"\n",
        "def calcAlpha1(x,y,W,W_pred):\n",
        "    xv = np.dot(x,W_pred-W)\n",
        "    xtxv = np.dot(x.T,xv)\n",
        "    num = 2 * np.linalg.norm(xv)**2\n",
        "    den = np.linalg.norm(xtxv)**2\n",
        "    alpha = num/den\n",
        "    return alpha\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Computes approximate value of critical learning rate\n",
        "\n",
        "Parameters\n",
        "--------------------\n",
        "    x      -- numpy array of shape (n,d)\n",
        "    y      -- numpy array of shape (n,1)\n",
        "    W      -- numpy array of shape (d,1), true weights of the data distribution (not used)\n",
        "    W_pred -- numpy array of shape (d,1), estimated weights of the data distribution\n",
        "Returns\n",
        "--------------------\n",
        "    alpha  -- float, critical value of the learning rate (computed using approximations)\n",
        "\"\"\"\n",
        "def calcAlpha2(x,y,W,W_pred):\n",
        "    y_pred = np.dot(x,W_pred)\n",
        "    num = 2*(np.linalg.norm(y)**2 + np.linalg.norm(y_pred)**2 - 2*np.dot(y.T,y_pred))\n",
        "\n",
        "    xt_y = np.dot(x.T,y)\n",
        "    xt_y_pred = np.dot(x.T,y_pred)\n",
        "    den = np.linalg.norm(xt_y)**2 + np.linalg.norm(xt_y_pred)**2 - 2*np.dot(xt_y.T,xt_y_pred)\n",
        "\n",
        "    alpha = num/den\n",
        "    return alpha[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1qjdkZmQWkf"
      },
      "source": [
        "**Simulation 1**\n",
        "\n",
        "This simulation implements gradient descent with the approximate learning rate scheduler as well as the analytic solution for increasing values of the training data size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJs5KNNLutOC"
      },
      "source": [
        "n = 1500 # maximum number of training data points\n",
        "d = 1000 # dimension of input\n",
        "test_n = 400 # number of data points in test set\n",
        "numIters = 3000 # number of gradient descent updates\n",
        "r = 0.9 # parameter in the learning rate scheduler\n",
        "numSims = 1 # number of times to run the simulation\n",
        "\n",
        "W = np.random.uniform(0,1, size=(d,1))\n",
        "X_test,Y_test = generateData(test_n,d,W)\n",
        "\n",
        "numTrain = [i for i in range(1,n+1)]\n",
        "analyticRisks = [[0 for i in range(n)] for j in range(numSims)]\n",
        "gdRisks = [[0 for i in range(n)] for j in range(numSims)]\n",
        "\n",
        "pb1 = progressbar.ProgressBar()\n",
        "\n",
        "for sim in pb1(range(numSims)):\n",
        "    \n",
        "    X,Y = generateData(n,d,W) # Generate training data for every simulation\n",
        "    \n",
        "    pb2 = progressbar.ProgressBar()\n",
        "    for i in pb2(range(1,n+1)):\n",
        "        # Extract only i training samples\n",
        "        x = X[:i,] \n",
        "        y = Y[:i,]\n",
        "\n",
        "        # Analytic Solution\n",
        "        W_pred = np.dot(np.linalg.pinv(x),y)\n",
        "        Y_test_pred = np.dot(X_test, W_pred)\n",
        "        analyticRisks[sim][i-1] = (1/test_n) * np.linalg.norm(Y_test - Y_test_pred)\n",
        "\n",
        "        # Gradient Descent Solution\n",
        "        W_pred = np.zeros((d,1))\n",
        "        for numIter in range(numIters):\n",
        "            alpha = r * calcAlpha2(x,y,W,W_pred)\n",
        "            W_pred = W_pred - alpha * np.dot(x.T, np.dot(x,W_pred) - y)\n",
        "        Y_test_pred = np.dot(X_test,W_pred)\n",
        "        gdRisks[sim][i-1] = (1/test_n) * np.linalg.norm(Y_test - Y_test_pred)\n",
        "\n",
        "analyticRisksMean = np.mean(analyticRisks, axis=0)\n",
        "gdRisksMean = np.mean(gdRisks, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRY6T6SE5Dzn"
      },
      "source": [
        "# Plotting test risk against number of training samples\n",
        "\n",
        "plt.title('Test Risk v/s Number of Training Samples, d = ' + str(d))\n",
        "plt.plot(analyticRisksMean[:], label='Analytic Test Risk')\n",
        "plt.plot(gdRisksMean[:], label = 'GD Test Risk')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psG3jCINRZtu"
      },
      "source": [
        "**Simulation 2**\n",
        "\n",
        "This implements the learning rate scheduler (exact or approximate as desired) along with the analytical solution for a selected value of n and d."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K02ZbwqrPC5j"
      },
      "source": [
        "#####################################################################################\n",
        "# Setting Initial Conditions\n",
        "# Vary values as desired\n",
        "\n",
        "# Set to true to simulate exact learning rate scheduler\n",
        "# Set to false to simulate the approximate learning rate scheduler\n",
        "isExact = False\n",
        "\n",
        "d = 1000 # Data Dimension\n",
        "gamma = 1.15 # Underparametrization Ratio, gamma = 1 implies N = d\n",
        "N = int(gamma * d) # Number of training\n",
        "test_n = 500 # Size of test set in the simulations\n",
        "numSims = 1 # Number of simulations to average results over\n",
        "numIters = 3000 # Number of iterations in gradient descent\n",
        "alpha_factors = [0.9,1,1.1] # Corresponds to r in the paper\n",
        "zero_weights = True # False corresponds to Xavier initialization\n",
        "doPlots = True # True if you want to display the plots\n",
        "savePlots = False # True if you want to save the plots in addition to displaying it\n",
        "#####################################################################################\n",
        "\n",
        "\n",
        "# Handle whether to use exact or approximate learning rate scheduler\n",
        "def chooseMethod(isExact):\n",
        "    x = None\n",
        "    method = None\n",
        "    calcAlpha = None\n",
        "\n",
        "    if isExact:\n",
        "        method = 'Exact '\n",
        "        x = '1'\n",
        "        calcAlpha = calcAlpha1\n",
        "    else:\n",
        "        method = 'Approx. '\n",
        "        x = '2'\n",
        "        calcAlpha = calcAlpha2\n",
        "    return [method, x, calcAlpha]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyyUpqOkPTbU"
      },
      "source": [
        "method, x, calcAlpha = chooseMethod(isExact)\n",
        "\n",
        "W = np.random.normal(0,1, size=(d,1)) # Initializing true weights for simulation\n",
        "\n",
        "X_test, Y_test = generateData(test_n,d,W) # Generating test set\n",
        "\n",
        "numAlphas = len(alpha_factors)\n",
        "\n",
        "iters = [iter for iter in range(1,numIters+2)]\n",
        "\n",
        "test_errors_all = np.zeros((numAlphas,numIters+1))\n",
        "alphas_all = np.zeros((numAlphas,numIters))\n",
        "\n",
        "pb1 = progressbar.ProgressBar()\n",
        "for sim in pb1(range(numSims)):\n",
        "    alphas = [[None for iter in range(numIters)] for i in range(numAlphas)]\n",
        "    test_errors = [[None for iter in range(numIters+1)] for i in range(numAlphas)]\n",
        "    X_train,Y_train = generateData(N,d,W) # Generating new training data for every sim\n",
        "    for i in range(numAlphas):\n",
        "    # Either zero initialization or Xavier initialization\n",
        "        W_pred = None\n",
        "        if zero_weights:\n",
        "            W_pred = np.zeros((d,1))\n",
        "        else:\n",
        "            W_pred = np.random.normal(0,1/(d**0.5), size=(d,1))\n",
        "        pb2 = progressbar.ProgressBar()\n",
        "\n",
        "        for iter in range(numIters):\n",
        "            alpha = alpha_factors[i] * calcAlpha(X_train,Y_train,W,W_pred)\n",
        "            alphas[i][iter] = alpha\n",
        "            W_pred = W_pred - alpha * np.dot(X_train.T, np.dot(X_train,W_pred) - Y_train)\n",
        "            Y_test_pred = np.dot(X_test,W_pred)\n",
        "            test_error = (1/test_n) * np.linalg.norm(Y_test - Y_test_pred)\n",
        "            test_errors[i][iter] = test_error\n",
        "    \n",
        "        W_pred_2 = np.dot(np.linalg.pinv(X_train),Y_train)\n",
        "        Y_test_pred = np.dot(X_test,W_pred_2)\n",
        "        test_error = (1/test_n) * np.linalg.norm(Y_test - Y_test_pred)\n",
        "        test_errors[i][numIters] = test_error\n",
        "  \n",
        "    test_errors = np.array(test_errors)\n",
        "    test_errors_all = test_errors_all + test_errors\n",
        "    alphas_all = alphas_all + np.array(alphas)\n",
        "\n",
        "test_errors_all = (1/numSims) * test_errors_all\n",
        "alphas_all = (1/numSims) * alphas_all\n",
        "\n",
        "analytic_risk = np.mean(test_errors_all[:,-1])\n",
        "\n",
        "down = 0\n",
        "up = numIters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abc2DHzkSnB7"
      },
      "source": [
        "# Plotting test risk and optimal learning rate against number of gradient descent updates\n",
        "\n",
        "down = 0\n",
        "up = numIters\n",
        "if doPlots:  \n",
        "    # Plotting Test Errors\n",
        "    for i in range(numAlphas-1):\n",
        "        plt.plot(iters[down:up],test_errors_all[i][down:up],label='r = ' + str(alpha_factors[i]))\n",
        "    plt.hlines(analytic_risk,down,up,label='Closed Form Solution',color='red')\n",
        "    plt.title(method + 'learning rate scheduler (n = ' + str(N) + ', d = ' + str(d) + ')')\n",
        "    plt.xlabel('Number of Iterations')\n",
        "    plt.ylabel('Test Risk')\n",
        "    plt.legend()\n",
        "    if savePlots: plt.savefig('fig' + x + 'a.png')\n",
        "    plt.show()\n",
        "\n",
        "    up = 1500\n",
        "    plt.plot(iters[down:up],test_errors_all[numAlphas-1][down:up],label='r = ' + str(alpha_factors[numAlphas-1]),color='green')\n",
        "    plt.title(method + 'learning rate scheduler (n = ' + str(N) + ', d = ' + str(d) + ')')\n",
        "    plt.xlabel('Number of Iterations')\n",
        "    plt.ylabel('Test Risk')\n",
        "    plt.legend()\n",
        "    if savePlots: plt.savefig('fig' + x + 'b.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Plotting Learning Rates\n",
        "    up = numIters\n",
        "    for i in range(numAlphas-1):\n",
        "        plt.plot(iters[down:up],alphas_all[i][down:up],label='r = ' + str(alpha_factors[i]))\n",
        "    plt.title(method + 'learning rate scheduler (n = ' + str(N) + ', d = ' + str(d) + ')')\n",
        "    plt.xlabel('Number of Iterations')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.legend()\n",
        "    if savePlots: plt.savefig('fig' + x + 'c.png')\n",
        "    plt.show()\n",
        "\n",
        "    up = 1500\n",
        "    plt.plot(iters[down:up],alphas_all[numAlphas-1][down:up],label='r = ' + str(alpha_factors[numAlphas-1]),color='green')\n",
        "    plt.title(method + 'learning rate scheduler (n = ' + str(N) + ', d = ' + str(d) + ')')\n",
        "    plt.xlabel('Number of Iterations')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.legend()\n",
        "    if savePlots: plt.savefig('fig' + x + 'd.png')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}